问题：如何利用python-scrapy进行动态网站的抓取

1. 明白什么是动态网站？

	a) 动态网页: 指跟静态网页相对的一种网页编程技术;

	b) 静态网页: 随着html代码的生成，页面的内容和显示效果就基本上不会发生变化了——除非你修改页面代码

	c) 可以通过以下方式简单验证某网页是否为动态网页：
		在页面上右键查看源代码，和右键审查元素所看到的html代码是不一样的，如果后者中能看到，而前者没有的话，就说明这个页面是动态生成的---这个区别方式是很准确的。

2. Selenium的介绍：

	a) Selenium是Thoughtworks公司的一个集成测试的强大工具。

	b) Selenium 是 ThoughtWorks 专门为 Web 应用程序编写的一个验收测试工具。

	c) 使用Selenium 的最大好处是: Selenium测试直接在浏览器中运行，就像真实用户所做的一样。在浏览器加载js后，便可以通过xpath来解析网页了。

	d) 可以用pip install 或 easy_install 安装 Selenium package：pip install -U selenium

3. 利用Selenium获取后：self.browser.get(response.url)，测试过程中并没有改变response中的内容，只是有些动态网站中的属性值和网页源代码中的值不一致；

4. 如何进行js网站的抓取呢？

	a) 在ipython中可以获取相关帮助文档：
		In [1]: from selenium import webdriver
		In [2]: help(webdriver.Firefox)，其中的相关方法可以帮助我们获取js中相关的元素后者元素中的内容；
		我使用了self.browser.find_elements_by_xpath这个方法；

	b) 需要注意的是：有些html元素设置的属性：display：none，这在获取时会出现空的状况（无论是获取的text或者是运用点击属性click) 

		这里需要利用修改其js的属性来获取相关内容：
			jsScript= 'document.getElementById("carTypeList").style="display:block"'
			self.browser.execute_script(jsScript)
			ps:修改为block属性后，使用click属性会back；
				所以把修改属性放在了循环内

5. 如何提高效率

	对javascript的支持有四种解决方案:

	a) 写代码模拟相关js逻辑---进一步确认;

	b) 调用一个有界面的浏览器,类似各种广泛用于测试的,selenium---firefox,chrome等;

	d) 使用一个无界面的浏览器,各种基于webkit的phantomjs;

		1) phantomjs的使用类似于firefox;

		2) 主要涉及安装问题：

			下载页面：http://phantomjs.org/download.html

		3) tar -jvxf 安装包.tar.bz2;

		4) 无需另外安装，解压缩后显示可执行文件，添加至linux系统环境变量中即可；

		5) 运行爬虫检验有效性。

	e) 结合一个js执行引擎,自己实现一个轻量级的浏览器.难度很大.

6. 明确xpath选择的选择路径
	w3cschool中的xpath教学，选择器也要进一步学习，选择器包括(response和self.browser中的选择，两者的使用方法是不同的, 学会用yield)

	a)xpath的使用技巧

		如果html结构是这样:
		<div class="cowtc"></div>
		那么我知道可以写xpath //div[@class="cowtc"]

		但是如果我的html是
		<div class="test cowtc"></div>
		<div class="test cowtc yeah"></div>

		如何选出有"test cowtc"这个class的对象:
		利用contains技巧---//div[contains(@class, 'test cowtc')]

		如果是多个则可以：
		//div[contains(@class, 'cowtc1') and contains(@class, 'cowtc2')]

7.数据的存储
	1) 在items.py里面定义我们要抓取的数据：

		from scrapy.item import Item, Field

		class PeopleItem(Item):
			name = Field()
			age = Field()
			gender = Field()

		这里我们需要获取dmoz页面上的标题, 链接, 描述, 所以定义一个对应的items结构，不像Django里面models的定义有那么多种类的Field，这里只有一种就叫Field()，再复杂就是Field可以接受一个default值。

	2) spider中赋值items

		利用xpath或者css选择器进行相关的赋值

	3) 保存抓取得到的数据

		a) scrapy提供了几个选项，可以将数据保存为json，csv或者xml文件，执行的命令：
				scrapy crawl dmoz.org --set FEED_URI=items.json --set FEED_FORMAT=json

		b) 如果需要对items数据进一步处理，比如直接保存到数据库，就要用到pipelines.py文件
