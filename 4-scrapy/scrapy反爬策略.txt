声明:
	爬虫只为Geek玩用，不进行商用，违者后果自负。

爬虫的识别
	参考: http://blog.csdn.net/gaojava/article/details/8593436

1. 端口的占用量来识别爬虫

2. user-agent来识别爬虫

3. 网站流量统计来识别爬虫

	主流的网站流量统计系统不外乎两种实现策略：一种策略是在网页里面嵌入一段js，这段js会向特定的统计服务器发送请求的方式记录访问量；另一种策略是直接分析服务器日志，来统计网站访问量。在理想的情况下，嵌入js的方式统计的网站流量应该高于分析服务器日志，这是因为用户浏览器会有缓存，不一定每次真实用户访问都会触发服务器的处理。但实际情况是，分析服务器日志得到的网站访问量远远高于嵌入js方式，极端情况下，甚至要高出10倍以上。

	现在很多网站喜欢采用awstats来分析服务器日志，来计算网站的访问量，但是当他们一旦采用Google Analytics来统计网站流量的时候，却发现GA统计的流量远远低于awstats，为什么GA和awstats统计会有这么大差异呢？罪魁祸首就是把自己伪装成浏览器的网络爬虫。这种情况下awstats无法有效的识别了，所以awstats的统计数据会虚高。

	其实作为一个网站来说，如果希望了解自己的网站真实访问量，希望精确了解网站每个频道的访问量和访问用户，应该用页面里面嵌入js的方式来开发自己的网站流量统计系统。自己做一个网站流量统计系统是件很简单的事情，写段服务器程序响应客户段js的请求，分析和识别请求然后写日志的同时做后台的异步统计就搞定了。

	通过流量统计系统得到的用户IP基本是真实的用户访问，因为一般情况下爬虫是无法执行网页里面的js代码片段的。所以我们可以拿流量统计系统记录的IP和服务器程序日志记录的IP地址进行比较，如果服务器日志里面某个IP发起了大量的请求，在流量统计系统里面却根本找不到，或者即使找得到，可访问量却只有寥寥几个，那么无疑就是一个网络爬虫

4. 实时反爬虫防火墙阻止爬虫
	 通过分析日志的方式来识别网页爬虫不是一个实时的反爬虫策略。如果一个爬虫非要针对你的网站进行处心积虑的爬取，那么他可能会采用分布式爬取策略，比方说寻找几百上千个国外的代理服务器疯狂的爬取你的网站，从而导致网站无法访问，那么你再分析日志是不可能及时解决问题的。所以必须采取实时反爬虫策略，要能够动态的实时识别和封锁爬虫的访问。

	要自己编写一个这样的实时反爬虫系统其实也很简单。比方说我们可以用memcached来做访问计数器，记录每个IP的访问频度，在单位时间之内，如果访问频率超过一个阀值，我们就认为这个IP很可能有问题，那么我们就可以返回一个验证码页面，要求用户填写验证码。如果是爬虫的话，当然不可能填写验证码，所以就被拒掉了，这样很简单就解决了爬虫问题。

5. 登录入口阻止爬虫
	2016年12月份的知乎，该网站的入口是登录页面，需要帐号和验证，存在登录页面问题是不大的，因为爬虫可以模拟登录页面，但是问题在于验证码上，只有通过验证才能登录成功，该验证码的破解是需要费功夫的。
	另外一种在此环境下进行爬虫的方法是，手动登录，之后利用登录后的cookies进行爬虫的登录，这种cookies的时间是有限制的。

6. 验证码阻止爬虫
	当服务器端在判定爬虫的结果上有犹豫时，会抛出验证页面来进行反爬。

攻破反爬手段
1. robots协议
	Robots协议（也称为爬虫协议、机器人协议等）的全称是“网络爬虫排除标准”（Robots Exclusion Protocol），网站通过Robots协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取。
	在进行某些web抓取过程中，需要禁用robots协议，虽然是不道德的，但是是为Geek所测试。

2. user-agent
	一些网站常常通过判断 UA 来给不同的操作系统、不同的浏览器发送不同的页面，因此可能造成某些页面无法在某个浏览器中正常显示，但通过伪装 UA 可以绕过检测。

3. 模拟登录页面

4. 个性化设定爬取
	随机时间段，限制一定的速度来进行爬取，需要尽量模拟正常人的上网时间和速度。

5. 代理IP的使用

	5.1 代理IP的分类
		1) 高匿名：服务器不知道你使用了代理ip和不知道你真实ip
		2) 匿名：服务器知道你使用了代理ip但不知道你的真实ip
		3) 透明：服务器知道你使用了代理ip而且知道你真实ip

	5.2 代理IP的使用方法

	5.3 代理IP的维护方法
		涉及到Redis和MongoDB的学习。		

6. 验证码破解
	
	6.1) 是否可以利用网站cookies继续爬取;

		利用cookies登录网页，我们不需要知道登录url和表单字段以及其他参数，不需要了解登录的过程和细节。

		由于不是采用登录url, 用户名+密码的方式。配合工具使用，快速方便。

		所谓用Cookie实现登录，就把过登录过的信息（包括用户名、密码以及其他的验证信息）打包一起发给服务器，告诉服务器我是登录验证过的。

		不足之处，Cookie有过期时间，过一段时间再运行这个爬虫，需要重新获取一下Cookie的值。抓取数据过程是没有问题的。

	关于Cookie的介绍：

    1. Cookie分类
		Cookie总是保存在用户客户端中，按在客户端中的存储位置，可分为内存Cookie和硬盘Cookie。Cookie的有效性，最短的浏览器关闭后就消失了，最长是可以一直保存，直到被删除。

	2. Cookie用途
		因为HTTP协议是无状态的，即服务器不知道用户上一次做了什么，这严重阻碍了交互式Web应用程序的实现。
		在典型的应用是网上购物场景中，用户浏览了几个页面，买了一盒饼干和两饮料。最后结帐时，由于HTTP的无状态性，不通过额外的手段，服务器并不知道用户到底买了什么。
		所以Cookie就是用来绕开HTTP的无状态性的“额外手段”之一。服务器可以设置或读取Cookies中包含信息，借此维护用户跟服务器中的状态。

	3. Cookie的缺陷
		1）Cookie会被附加在每个HTTP请求中，所以无形中增加了流量。
		2) 由于在HTTP请求中的Cookie是明文传递的，所以安全性成问题。（除非用HTTPS）
		3) Cookie的大小限制在4KB左右。对于复杂的存储需求来说是不够用的。

	6.2) 这块需要锻炼问题的解决能力和学习图像处理的算法，利用python现有的工具包，在某种程度上是可以克服验证码的问题的;
