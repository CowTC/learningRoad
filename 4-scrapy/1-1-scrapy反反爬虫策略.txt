声明:
	爬虫只为Geek玩用，不进行商用，违者后果自负。

攻破反爬手段

1. Robots协议

	Robots协议(也称为爬虫协议、机器人协议等)的全称是"网络爬虫排除标准"(Robots Exclusion Protocol), 网站通过Robots协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取。

	在进行某些web抓取过程中，需要禁用robots协议，虽然是不道德的，但是是为Geek所测试。

2. user-agent
	一些网站常常通过判断 UA 来给不同的操作系统、不同的浏览器发送不同的页面，因此可能造成某些页面无法在某个浏览器中正常显示，但通过伪装 UA 可以绕过检测。

3. 模拟登录页面

	a) 利用表单，验证登录

		传统的登陆页面，进行用户名和密码的填写后post提交登陆;

	b) cookies的使用

		利用cookies登录网页，我们不需要知道登录url和表单字段以及其他参数，不需要了解登录的过程和细节. 所谓用Cookie实现登录，就把过登录过的信息 (包括用户名、密码以及其他的验证信息) 打包一起发给服务器，告诉服务器我是登录验证过的。

		不足之处，Cookie有过期时间，过一段时间再运行这个爬虫，需要重新获取一下Cookie的值。抓取数据过程是没有问题的。

4. 个性化设定爬取
	随机时间段，限制一定的速度来进行爬取，需要尽量模拟正常人的上网时间和速度。

5. 代理IP的使用

	a) 代理IP的分类
		1) 高匿名：服务器不知道你使用了代理ip和不知道你真实ip
		2) 匿名：服务器知道你使用了代理ip但不知道你的真实ip
		3) 透明：服务器知道你使用了代理ip而且知道你真实ip

	b) 代理IP的使用方法

	c) 代理IP池的维护方法

6. 验证码破解
	a) 这块需要锻炼问题的解决能力和学习图像处理的算法，利用python现有的工具包，在某种程度上是可以克服验证码的问题的;
